{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B Final Project\n",
    "### Authors:\n",
    "- Michelle Tran\n",
    "- Christopher Ly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color:red\">Just for reference for working on GitHub:</strong>\n",
    "<p style=\"color:red\">Be sure to clear output for the notebook before pushing to the repo, this is to keep commit history clean. You can do this by following the sequence below:</p>\n",
    "\n",
    "`Cell > All Output > Clear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'src')\n",
    "import utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='darkgrid')#, palette='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 'data/telco.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(fp).drop(['customerID'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_etl = {\n",
    "    'Num of features': df.shape[1] - 1, # Churn is a label not a feature\n",
    "    'Num of datapoints': df.shape[0],\n",
    "    'Num of not churned': sum(df['Churn']=='No'),\n",
    "    'Num of churned': sum(df['Churn']=='Yes'),\n",
    "    '% churned': np.mean(df['Churn']=='Yes')*100\n",
    "}\n",
    "summary = pd.DataFrame(bf_etl, index=['Before etl'])\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df['tenure']==0].index).reset_index(drop=True)\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'])#, errors='coerce')\n",
    "df.head(2)\n",
    "# df['tenure'] = df['tenure'].replace({0: 1})\n",
    "\n",
    "# df['TotalCharges'] = df['TotalCharges'].fillna(df['MonthlyCharges'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.revert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://stackoverflow.com/questions/33179122/seaborn-countplot-with-frequencies\n",
    "ax = sns.countplot(x=df['Churn'])\n",
    "plt.title('Churn rate/occurrence')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_ylim(0, len(df))\n",
    "\n",
    "for p in ax.patches:\n",
    "    x=p.get_bbox().get_points()[:,0]\n",
    "    y=p.get_bbox().get_points()[1,1]\n",
    "    ax.annotate('{:.2f}%'.format(100.*y/len(df)), (x.mean(), y), \n",
    "            ha='center', va='bottom') # set the alignment of the text\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_cat(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['tenure'][df['Churn'] == 'Yes'])\n",
    "sns.distplot(df['tenure'][df['Churn'] == 'No'])\n",
    "plt.title('Density of Tenure in Months for Churn and No Churn')\n",
    "plt.legend(['Churn', 'No Churn'])\n",
    "plt.xlabel('Tenure (Months)')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['MonthlyCharges'][df['Churn'] == 'Yes'])\n",
    "sns.distplot(df['MonthlyCharges'][df['Churn'] == 'No'])\n",
    "plt.title('Density of Monthly Charges for Churn and No Churn')\n",
    "plt.legend(['Churn', 'No Churn'])\n",
    "plt.xlabel('Monthly Charges')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['TotalCharges'][df['Churn'] == 'Yes'])\n",
    "sns.distplot(df['TotalCharges'][df['Churn'] == 'No'])\n",
    "plt.title('Density of Total Charges for Churn and No Churn')\n",
    "plt.legend(['Churn', 'No Churn'])\n",
    "plt.xlabel('Total Charges')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addOns = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n",
    "def internet_add_ons(r):\n",
    "    if r['InternetService'] == 'No':\n",
    "        return 0\n",
    "    count = 0\n",
    "    for i in addOns: \n",
    "        if r[i] == 'Yes':\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['InternetAddOns'] = df.apply(internet_add_ons, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Replaces other distplots, requires `pip install --upgrade seaborn`\n",
    "utils.plot_num(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['InternetAddOns'][df['Churn'] == 'Yes'])\n",
    "sns.distplot(df['InternetAddOns'][df['Churn'] == 'No'])\n",
    "plt.title('Density of Internet Add Ons for Churn and No Churn')\n",
    "plt.legend(['Churn', 'No Churn'])\n",
    "plt.xlabel('Internet Add Ons')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.convert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_categorical = df.select_dtypes(include=object).drop(addOns, axis=1)\n",
    "df_numerical = df.select_dtypes(include=np.number).drop(['Churn', 'TotalCharges'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(ohe.fit_transform(df_categorical).todense(), columns=ohe.get_feature_names(df_categorical.columns))\n",
    "X = pd.concat([df_numerical,X], axis=1)\n",
    "y = df['Churn']\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_etl = {\n",
    "    'Num of features': X.shape[1],\n",
    "    'Num of datapoints': X.shape[0],\n",
    "    'Num of not churned': sum(df['Churn']==0),\n",
    "    'Num of churned': sum(df['Churn']),\n",
    "    '% churned': np.mean(df['Churn'])*100\n",
    "}\n",
    "summary.append(pd.DataFrame(af_etl, index=['After etl']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_spt = {\n",
    "    'Num of datapoints': [X_train.shape[0], X_test.shape[0]],\n",
    "    'Num of not churned': [sum(y_train==0), sum(y_test==0)],\n",
    "    'Num of churned': [sum(y_train), sum(y_test)],\n",
    "    '% churned': [np.mean(y_train)*100, np.mean(y_test)*100]\n",
    "}\n",
    "pd.DataFrame(af_spt, index=['Train data', 'Test data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://stackoverflow.com/questions/29432629/plot-correlation-matrix-using-pandas\n",
    "corr = df_numerical.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ix = ['Logistic Regression', 'SVM', 'AdaBoost']\n",
    "ave = 'binary'             # average parameter for F1 score\n",
    "beta = 5                   # beta parameter for Fβ score\n",
    "scr = 'balanced_accuracy'  # scoring parameter for grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\n",
    "    'Accuracy before grid search': [0,0,0],\n",
    "    'Accuracy after': [0,0,0],\n",
    "    'F1 Score': [0,0,0],\n",
    "    'Fβ Score': [0,0,0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = LogisticRegression(solver='liblinear')\n",
    "result = mdl.fit(X_train, y_train)\n",
    "score = mdl.score(X_test, y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.001,0.005,0.01,0.05,0.1,0.5,1,10], \n",
    "              'penalty' : ['l1', 'l2'],\n",
    "              'tol' : [1e-4],\n",
    "              'max_iter' : [100,500,1000]}\n",
    "gs = GridSearchCV(mdl, param_grid, n_jobs=-1, cv=10, scoring=scr)\n",
    "gs.fit(X_train, y_train);\n",
    "mdl = mdl.set_params(**gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_, gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['Accuracy before grid search'][0] = score\n",
    "res['Accuracy after'][0] = mdl.score(X_test, y_test)*100\n",
    "res['F1 Score'][0] = f1_score(y_test, mdl.predict(X_test), average=ave)\n",
    "res['Fβ Score'][0] = fbeta_score(y_test, mdl.predict(X_test), average=ave, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_sort = {k:v for k,v in zip(X.columns.values,mdl.coef_.reshape((-1,)))}\n",
    "sort = {k: v for k, v in sorted(to_sort.items(), key=lambda item: item[1])}\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.suptitle(\"Weights of logistic regression model\")\n",
    "sns.barplot(x=[x for _,x in sort.items()], y=[y for y,_ in sort.items()]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl2 = SVC(gamma='auto')\n",
    "result2 = mdl2.fit(X_train, y_train)\n",
    "score2 = result2.score(X_test, y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid2 = {'C': [0.001,0.005,0.01,0.05,0.1,0.5,1,10], \n",
    "               'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "               'gamma': ['auto', 'scale'],\n",
    "               'max_iter' : [100,500,1000]}\n",
    "gs2 = GridSearchCV(result2, param_grid2, n_jobs=-1, cv=10, scoring=scr)\n",
    "gs2.fit(X_train, y_train);\n",
    "mdl2 = mdl2.set_params(**gs2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2.best_params_, gs2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['Accuracy before grid search'][1] = score2\n",
    "res['Accuracy after'][1] = mdl2.score(X_test, y_test)*100\n",
    "res['F1 Score'][1] = f1_score(y_test, mdl2.predict(X_test), average=ave)\n",
    "res['Fβ Score'][1] = fbeta_score(y_test, mdl2.predict(X_test), average=ave, beta=beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl3 = AdaBoostClassifier()\n",
    "result3 = mdl3.fit(X_train, y_train)\n",
    "score3 = result3.score(X_test, y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid3 = {'n_estimators': [10,25,50,100,250], \n",
    "               'learning_rate' : [0.01,0.05,0.1,0.5,1,10]}\n",
    "gs3 = GridSearchCV(result3, param_grid3, n_jobs=-1, cv=10, scoring=scr)\n",
    "gs3.fit(X_train, y_train);\n",
    "mdl3 = mdl3.set_params(**gs3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs3.best_params_, gs3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['Accuracy before grid search'][2] = score3\n",
    "res['Accuracy after'][2] = mdl3.score(X_test, y_test)*100\n",
    "res['F1 Score'][2] = f1_score(y_test, mdl3.predict(X_test), average=ave)\n",
    "res['Fβ Score'][2] = fbeta_score(y_test, mdl3.predict(X_test), average=ave, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_sort = {k:v for k,v in zip(X.columns.values,mdl3.feature_importances_)}\n",
    "sort = {k: v for k, v in sorted(to_sort.items(), key=lambda item: item[1])}\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.suptitle(\"Weights of AdaBoost model\")\n",
    "sns.barplot(x=[x for _,x in sort.items()], y=[y for y,_ in sort.items()]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_sort = {k:v for k,v in zip(X.columns.values,mdl3.feature_importances_)}\n",
    "sort = {k: v for k, v in sorted(to_sort.items(), key=lambda item: item[1]) if v!=0}\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.suptitle(\"Weights of AdaBoost model\")\n",
    "sns.barplot(x=[x for _,x in sort.items()], y=[y for y,_ in sort.items()]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res, index=res_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mdl.predict(X_test) != mdl2.predict(X_test))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mdl.predict(X_test) != mdl3.predict(X_test))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mdl2.predict(X_test) != mdl3.predict(X_test))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
